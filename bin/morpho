#!/usr/bin/env python2.7
from itertools import product, chain
from collections import defaultdict
from multiprocessing.dummy import Pool
from subprocess import call
import os, errno, random, glob, sys
import os.path
import argparse

# infrastructure 

def mkdir_p(path):
  try:
    os.makedirs(path)
  except OSError as exc: # Python >2.5
    if exc.errno == errno.EEXIST and os.path.isdir(path):
      pass
    else: raise

class datafile: 
  def __init__(self, path): 
    self.path = path 
    self.abspath = os.path.abspath(path)
    self.basename = os.path.basename(path)
    self.dirname = os.path.dirname(path)
    self.stem = os.path.splitext(self.basename)[0]
  def exists(self): 
    return os.path.isfile(self.realpath)
  def __str__(self):
    return self.abspath

class out(datafile):
  pass

class image(datafile):
  def objects(self):
    return map(datafile, glob.glob('{0.dirname}/../objects/*.obj'.format(self)))
  def labels(self):
    return map(datafile, 
        glob.glob('{0.dirname}/../labels/{0.stem}_labels.mnc'.format(self)))
  def __eq__(self, other):
    if isinstance(other,self.__class__):
      return os.path.realpath(self.abspath) == os.path.realpath(other.abspath)
    else:
      return false
  def __ne__(self,other):
    return not self.__eq__(other)

class command:
  def __init__(self, fmt_string, *args, **kwargs):
    cmdstr = fmt_string.format(*args,**kwargs)
    self.__init__(shlex.split(cmdstr))  # split it up into parts
    #
    #todo: we could augment the usual string formatting syntax to include 
    #      the ability to indicate types (e.g. that an argument is an 'out'put
    #      file. 
    #
    #      Ideas: 
    #         # 1. use <Type>@<string> to mean 
    #         #   call Type(string), as in: 
    #         nuc_correct {0} out@{output_dir}/nuc/{0.stem} 
    #
    #         # Q: what if there are @ characters in the string?
    #         # A: too bad. :-)  But honestly, we could just check for strings
    #         that match $\w+@ so that the only strings that may get confused
    #         are those that happen to start with out@ and are NOT mean to be
    #         interpreted as types. 
    #
    #         # 2. use the conversion option for output files: o
    #         bestlinreg {from} {to} {xfm!o} {resampled!o}

  def __init__(self, *args): 
    #todo: sanity checks
    self.cmd = args

class taskset:
  def __init__(self):
    self.stages = defaultdict(list)
    self.stage_order = []
  def stage(self,stage_name):
    """syntactic sugar"""
    t = self
    class _stage(): 
      def command(self, *args):
        t.command(stage_name,args)
    return _stage()
  def command(self,stage,cmd):
    self.stages[stage].append(cmd)
    if stage not in self.stage_order: self.stage_order.append(stage)
  def set_stage_order(self, order):
    assert set(order).issubset(set(self.stages.keys)), "some stages given are not part of this taskset"
    self.stage_order = order
  def __str__(self):
    __str=""
    for stage in self.stage_order:
      __str += '{0}:\n'.format(stage)
      for command in self.stages[stage]:
        __str += '\t{0}\n'.format(' '.join(map(str,command)))
    return __str

def run_command(command): 
  cmdstr = " ".join(map(str,command))
  print >> sys.stderr, 'COMMAND:', cmdstr
  return call(cmdstr,shell=True)

class multiprocqueue:
  def run(self,tasks,processes=None,stage_order=None):
    """Runs the commands from the given stages in the task list.
       
       If stages isn't provided, then all stages are run. """
    stage_order = stage_order or tasks.stage_order
    for stage in stage_order:
      print '## stage:', stage,'##'

      results = self._run(tasks.stages[stage], processes)
      print 'results ---',results
      #TODO: assert that there were no errors
      if not all(map(lambda x: x==0,results)):
        print "Error in stage {0}. Exiting...".format(stage)
        return -1

  def _run(self,commands,processes=None): 
    unfinished, outputs = self._filter_unfinished(commands)

    # make output dirs directories
    map(mkdir_p, set(map(lambda x: x.dirname, chain(*outputs))))

    return Pool(processes).map(run_command, unfinished)

  def _filter_unfinished(self,commands):
    """returns tuple of each unfinished command, and a list of its output files"""
    details = [(c,filter(lambda x: isinstance(x,out),c)) for c in commands]
    filtered = [(c,o) for (c,o) in details if not all(map(lambda x: os.path.isfile(str(x)),o))]
    if filtered:
        unfinished, outputs = zip(*filtered)
    else:
        unfinished, outputs = (), ()
    return (unfinished, outputs)

class Morphobits:
  def __init__(self):
    self._config = dict() # runtime configuration  #todo: temporary, replace
                                                   # with members

  def xfmpath(self,images,name='nl.xfm'):
    """Returns a standard path to an XFM file based on image names"""
    args=[self._config['output_dir'],self._config['reg_dir']] + [i.stem for i in images] + [name]
    return os.path.join(*args)

  def config(self, key, value=None):
    if value: 
      self._config[key] = value
    else:
      value = self._config.get(key,None)
    return value

  def build_pipeline(self,atlases,native_subjects,model,tasklist=None):
    tasklist = tasklist or taskset()
    subjects = []
    output_dir=self.config('output_dir')

    # syntactic sugar for now
    def stage(stage_name):
      return tasklist.stage(stage_name)
    
    def command(command,stage):
      tasklist.stage(stage).command(*command)

    ## BEGIN
    # move to subjects to model space 
    for s in native_subjects:
      nuc=out('output/nuc/'+s.basename)
      xfm=out(self.xfmpath([model,s],name='lsq9.xfm'))
      modelsubject=out('output/modelspace/'+s.basename)
      command(('nu_correct','-quiet',s,nuc), stage='subject.nuc')
      stage('subject.model.prop').command(
              'bestlinreg','-noverbose','-lsq9',nuc,model,xfm,modelsubject)
      subjects.append(image(modelsubject.path))
  
    # fetch templates specified on commandline
    templates = filter(lambda x: x.stem in self.config('templates'),subjects)

    # do pairwise registrations between atlases and templates and subjects 
    for x,y in chain(product(atlases,templates), product(templates,subjects)):
      stage('pairwise.reg').command('mb_register',x,y,out(self.xfmpath([x,y])))

    # do stuff with XFMs on all possible paths to subject
    for s in subjects:
      grids=list()
      objects=defaultdict(list)
      gridavg='{output_dir}/gridavg/{s.stem}_grid.mnc'.format(**vars())

      # compute all possible XFM paths to subject
      for a,t in product(atlases,templates):
        xfm=self.xfmpath([model,a,t,s])
        xfmdir=os.path.dirname(xfm)
        grid='{xfmdir}/grid.mnc'.format(**vars())
      
        # create single XFM for pathway (skip moves from identical images)
        xfms=[]
        for source,target in zip([model,a,t],[a,t,s]):
          if source != target:
            xfms.append(self.xfmpath([source,target]))
        assert xfms is not [], "{model}, {a}, {t}, {s} are all the same image?".format(**vars())
        xfms.append(out(xfm))
        stage('model.subject.xfm').command('xfmjoin',*xfms)

        # compute displacement along the pathway
        stage('model.subject.displace').command(
            'minc_displacement',model,xfm,out(grid))
          
        # transform objects from atlases
        if self.config('surface_area'):
          for o in a.objects():
            kind=o.stem
            object='${xfmdir}/${kind}'.format(**vars())
            stage('model.subject.objects.transform').command(
                'transform_objects',o,xfm,out(object))
            objects[kind].append(object)
        grids.append(grid)
    
      # compute the average XFM over all pathways to the subject
      grids.append(out(gridavg))
      stage('model.subject.gridavg').command('mincaverage',*grids)

      # calculate displacement of model objects 
      displacement_dir='{output_dir}/displacement'.format(**vars())
      for o in model.objects():
        obj_displace='{displacement_dir}/{s.stem}_{o.stem}.obj'.format(**vars())
        stage('model.subject.displacement').command(
                'object_volume_dot_product',o,gridavg,out(obj_displace))

      # operate on model..subject surfaces
      if self.config('surface_area'):
        for kind in objects.keys(): 
          # median surface per subject
          kind_dir  = '{output_dir}/{kind}'.format(**vars())
          medianobj = '{kind_dir}/{s.stem}_median.obj'.format(**vars())
          voronoi   = '{kind_dir}/{s.stem}_vorn.txt'.format(**vars())
          vorn_sa   = '{kind_dir}/{s.stem}_vorn_SA.txt'.format(**vars())

          stage('objects.median').command(
            "make_median_surfaces.pl",out(medianobj),*objects[kind])
          stage('objects.normals').command(
            "recompute_normals",medianobj,medianobj) #fix: not out()

          if self.config('voronoi'):
            # voronoi of median surface
            stage('objects.voronoi').command(
              'depth_potential', '-area_voronoi',medianobj,out(voronoi))

            #hack: determine the blurring kernal to use: 
            kernel = kind.startswith('gp_') and 3 or 5
            vorn_blur = '{kind_dir}/{s.stem}_vorn_SA_{kernel}mm_blur.txt'.format(
                **vars())

    return tasklist

def parse_args(): 
  parser = argparse.ArgumentParser()
  parser.add_argument("-n", dest="dry_run", default=False,
    action="store_true", help="Dry run. Show what would happen.")
  parser.add_argument("-j", "--processes", default=8, 
    type=int, metavar='N', help="Number of processes to parallelize over.")
  parser.add_argument("-s", '--subjects', nargs='+', default=None,
    help="A list of subjects to use (name of the brain image without .mnc extension)")
  parser.add_argument("-t", '--templates', nargs='+', default=(),
    help="A list of templates to use (name of the subject without .mnc extension)")
  parser.add_argument('--voronoi', default=False, action="store_true",
    help="Compute voronoi partition of the median surface.")
  parser.add_argument('--show-pipeline', default=False, action="store_true",
    help="Show pipeline.")
  return parser.parse_args()
  

def main():
  options = parse_args() 

  morpho  = Morphobits()
  morpho.config('output_dir','output')
  morpho.config('reg_dir',   'registrations')
  morpho.config('voronoi',   options.voronoi)

  atlases = map(image, glob.glob('input/atlases/brains/*.mnc'))
  subjects= map(image, glob.glob('input/subjects/brains/*.mnc'))
  model   = map(image, glob.glob('input/model/brains/*.mnc'))[0] # check there is only one!

  if options.subjects:
    subjects = [s for s in subjects if s.stem in options.subjects]
  
  #todo: sanity checks
  # - we have some subjects?
  # - templates are a subset of subjects?
  #todo: we should pass in templates to build_pipeline
  morpho.config('templates', options.templates)

  print 'atlases', ','.join(map(str,atlases))
  print 'subjects', ','.join(map(str,subjects))
  tasklist= morpho.build_pipeline(atlases,subjects,model)

  if options.dry_run:
    if options.show_pipeline:
      print tasklist
  else:
    queue = multiprocqueue()
    queue.run(tasklist,processes=2)

if __name__ == '__main__': 
  main()
